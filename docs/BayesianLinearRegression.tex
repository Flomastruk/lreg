\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{mathtools}

\usepackage{wrapfig, tikz, tikz-cd}
\usetikzlibrary{arrows, arrows, calc, decorations.markings, automata,calc}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=false,linkbordercolor=red,linkcolor=green,pdfborderstyle={/S/U/W 2}}

\usepackage{url}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\numberwithin{equation}{section}

\newcommand{\dd}{\partial}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\rr}{\text{r}}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\eps}{\varepsilon}
\newcommand{\ph}{\varphi}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\BB}{\mathfrak{B}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\SSigma}{\boldsymbol{\Sigma}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\MM}{\mathfrak{M}}
\newcommand{\kk}{\textbf{k}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\eq}{\stackrel{\mathclap{\normalfont\text{def}}}{=}}
\newcommand{\convas}{\stackrel{\mathclap{\normalfont\text{as}}}{\longrightarrow}}

\newcommand{\XTX}{X^TX}
\newcommand{\XTXi}{\left(X^TX\right)^{-1}}
\newcommand{\invGamma}{\text{Inv-}\Gamma}

\begin{document}
\title{Bayesian Linear Regression}
\author{Efim Abrikosov}
\maketitle

\section{Framework}
\subsection{Notations}

\begin{equation}\label{linear}
    y = x\cdot \bbeta +\varepsilon,\ \varepsilon \sim\mathcal{N}(0,\sigma^2)
\end{equation}

\begin{enumerate}
    \item Individual observations $(x, y) \in \RR^k\times \RR$
    \item Observed data $(X, Y)\in \RR^{n\times k}\times \RR^n$
    \item Linear regression weights $\bbeta \in \RR^k$ 
    \item Observation error variance $\sigma^2$
\end{enumerate}

\subsection{Model Assumptions}
\begin{enumerate}
    \item Observations $X$ have full rank
    \item Observation errors are independent, normally distributed with
    mean zero and variance $\sigma^2$
    \item Relation \ref{linear} holds
    \item Error variance $\sigma^2$ is either known, or its prior distribution
    is inverse gamma distribution with parameters $a_0,\ b_0$
    \item $\bbeta$ has a conditional prior distribution
    $\beta\mid\sigma^2\sim\mathcal{N}(\bbeta_0,\sigma^2\SSigma_0)$
\end{enumerate}

\section{Known Observation Variance with Linear Weights Prior}
\subsection{Summary of Results}
Posterior distribution of $\bbeta$:
\begin{flalign}
    \label{posterior_beta0}&f(\bbeta\mid Y,X)=\mathcal{N}(\bbeta_1, \sigma^2\SSigma_1)&&\\
    &\SSigma_1^{-1}=\SSigma_0^{-1}+\XTX &&\\
    &\bbeta_1=\SSigma_1\left(\XTX \widehat\bbeta + \SSigma_0^{-1}\bbeta_0\right)&&\\
    &\widehat{\bbeta}=\XTXi X^TY&&
\end{flalign}
Posterior prediction distribution $\widehat y \equiv x\cdot\bbeta$:
\begin{equation}\label{posterior_pred0}
    f(\widehat y\mid Y, X, x)= \mathcal{N}\left(x\bbeta_1, \sigma^2 x\XTXi x^T\right)
\end{equation}
Posterior observation distribution $\widehat y \equiv x\cdot\bbeta + e$:
\begin{equation}\label{posterior_obs0}
    f(y\mid Y, X, x)= \mathcal{N}\left(x\bbeta_1, \sigma^2\left(1+x\XTXi x^T\right)\right)
\end{equation}

[Meaning?][Confidence interval, for a fixed value $\underline{\smash{\bbeta}}$
and a linear constraint $\boldsymbol{c}\in\RR^k$:
\begin{equation}
    \frac{\boldsymbol{c}(\underline{\smash{\bbeta}}-\bbeta_1)}{\sigma\sqrt{\left(\boldsymbol{c}\SSigma_1 \boldsymbol{c}^T\right)}} \sim \mathcal{N}(0,1)
\end{equation}
Joint $f$-test for a set of linear constraints $\boldsymbol{C}\in\RR^{l \times k}$]


\subsection{Uninformative Prior}

With the prior $\Lambda_0 \equiv \SSigma_0^{-1} = 0$, the posterior
\ref{posterior_beta0} reduces to:
\begin{flalign}
    &f(\bbeta\mid Y,X)\sim \mathcal{N}(\bbeta_1, \sigma^2\SSigma_1)&&\\
    &\SSigma_1=\XTXi &&\\
    &\bbeta_1=\XTXi X^TY&&
\end{flalign}
which is the standard result obtained in classical OLS set up.

The standard prediction interval for $\widehat y(x)$ and the confidence interval
for an observation $y(x)$ follow from normal distributions in
\ref{posterior_pred0} and \ref{posterior_obs0} respectively.

\section{Conjugate Priors For Observation Variance and Linear Weights}
\subsection{Setup}
\begin{equation}
    f(Y,X\mid \bbeta,\sigma^2) = \left(2\pi\sigma^2\right)^{-\frac{n}{2}}
    \exp\left(-\frac{1}{2\sigma^2}(y-X\bbeta)^T(y-X\bbeta)\right)
\end{equation}
\begin{equation}
    f(\bbeta\mid\sigma^2)=\lvert2\pi\SSigma_0\rvert^{-1}
    \exp\left(-\frac{1}{2\sigma^2}(\bbeta-\bbeta_0)\SSigma_0^{-1}(\bbeta-\bbeta_0)^T\right)
\end{equation}
\begin{equation}
    f(\sigma^2)=\frac{b_0^{a_0}}{\Gamma(a_0)}(\sigma^2)^{-a_0-1}\exp\left(-\frac{b_0}{\sigma^2}\right)
\end{equation}
Alternatively $f(\sigma^2)$ can be written as scaled inverse chi-squared distribution
with parameters $\left(\nu_0,\tau_0^2\right)=\left(2a_0,\frac{b_0}{a_0}\right)$


\subsection{Summary of Results}
Posterior distribution of $\bbeta$:
\begin{flalign}
    \label{posterior_sigma}&f(\sigma^2\mid Y,X)=\invGamma(a_1,b_1)&&\\
    &a_1=a_0+\frac n 2&&\\
    &b_1=b_0+\frac 12\left(Y^TY+\bbeta_0\SSigma_0^{-1}\bbeta_0^T-\bbeta_1\SSigma_1^{-1}\bbeta_1^T\right)&&\\
    \label{posterior_beta}&f(\bbeta\mid Y,X,\sigma^2)=\mathcal{N}(\bbeta_1,\sigma^2\SSigma_1)&&\\
    &\SSigma_1^{-1}=\SSigma_0^{-1}+\XTX &&\\
    &\bbeta_1=\SSigma_1\left(\XTX \widehat\bbeta + \SSigma_0^{-1}\bbeta_0\right)&&\\
    &\widehat{\bbeta}=\XTXi X^TY&&
\end{flalign}
Posterior prediction distribution $\widehat y \equiv x\cdot\bbeta$:
\begin{equation}\label{posterior_pred}
    f(\widehat y\mid Y, X, x)\propto\left(1+\frac{a_1\left(y-x\bbeta_1\right)^2}{vb_1}\frac 1{2a_1}\right)^{-\frac{2a_1+1}{2}}
\end{equation}
\begin{equation}
    v = \left(1-x\left(\SSigma_1+x^Tx\right)^{-1}x^T\right)^{-1}
\end{equation}
This is Student's $t$-distribution on $y-x\bbeta_1$ with scale $\frac{vb_1}{a_1}$ and $2a_1$ degrees of freedom.

Posterior observation distribution $\widehat y \equiv x\cdot\bbeta + e$:
\begin{equation}\label{posterior_obs}
    f(y\mid Y, X, x)=??
\end{equation}


\end{document}