% \documentclass[12pt]{article}
\documentclass[10pt,fleqn]{amsart}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{mathtools}
% \usepackage{mathrsfs}
\usepackage{url}

\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-1.5in}
\calclayout


% \usepackage{wrapfig, tikz, tikz-cd}
% \usetikzlibrary{arrows, arrows, calc, decorations.markings, automata,calc}

% \usepackage{xcolor}
% \hypersetup{colorlinks=false,linkbordercolor=red,linkcolor=green,pdfborderstyle={/S/U/W 2}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\numberwithin{equation}{section}

% \newcommand{\al}{\alpha}
% \newcommand{\be}{\beta}
% \newcommand{\eps}{\varepsilon}
% \newcommand{\ph}{\varphi}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\aaa}{\boldsymbol{a}}
\newcommand{\BB}{\mathfrak{B}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\II}{\mathbb{I}}
% \newcommand{\kk}{\textbf{k}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\MM}{\mathfrak{M}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
% \newcommand{\rr}{\text{r}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\eq}{\stackrel{\mathclap{\normalfont\text{def}}}{=}}
\newcommand{\convas}{\stackrel{\mathclap{\normalfont\text{as}}}{\longrightarrow}}
\newcommand{\const}{\text{const}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\SSigma}{\boldsymbol{\Sigma}}
\newcommand{\llambda}{\boldsymbol{\lambda}}
\newcommand{\LLambda}{\boldsymbol{\Lambda}}
\newcommand{\Var}{\boldsymbol{V}}
\newcommand{\XTX}{X^TX}
\newcommand{\XTY}{X^TY}
\newcommand{\XTXi}{\left(X^TX\right)^{-1}}
\newcommand{\bbetahat}{\widehat{\bbeta}}
\newcommand{\bbetatilde}{\widetilde{\bbeta}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\ytilde}{\widetilde{y}}
\newcommand{\invGamma}{\text{Inv-}\Gamma}

\begin{document}
\title{Bayesian Linear Regression}
\author{Efim Abrikosov}
\date{\today}
\maketitle

\section{Framework}
\subsection{Notations}
\begin{equation}\label{linear}
    y = x\cdot \bbeta +\varepsilon,\ \varepsilon \sim\mathcal{N}\left(0,\sigma^2\right)
\end{equation}

\begin{enumerate}
    \item Individual observations $(x, y) \in \RR^k\times \RR$
    \item Observed data $(X, Y)\in \RR^{n\times k}\times \RR^n$
    \item Linear regression weights  $\bbeta \in \RR^k$
    \item Model parameter distribution mean $\mmu \in \RR^k$ and covariance matrix $\SSigma \in \RR^{k\times k}$
    \item Observation error variance $\sigma^2$
\end{enumerate}

\subsection{Model Assumptions}
\begin{enumerate}\label{assumptions}
    \item Observations $(x, y)$ satisfy linear relation \ref{linear}
    \item Observation errors are independent normally distributed with mean zero and variance $\sigma^2$
    \item For posterior estimation, observations $X$ must have full rank
    \item For known error variance $\sigma^2$, the prior on the space of parameters $\bbeta\in\RR^k$ is $\mathcal{N}\left(\mmu,\sigma^2\SSigma\right)$
    \item For unknown error variance, the prior for $\left(\bbeta, \sigma^2\right)\in\RR^{k+1}$ has $\sigma^2$ following inverse gamma distribution 
    with parameters $\left(a_0, b_0\right)$, and conditional distribution for linear relation weights $f\left(\bbeta\mid\sigma^2\right)=\mathcal{N}\left(\mmu,\sigma^2\SSigma\right)$
\end{enumerate}

\section{Gaussian Prior with Known Variance}
\subsection{Summary of Results}
\begin{proposition}[Posterior paramer distribution with known variance]\label{posterior}
    The posterior distribution of model parameters is normal $f(\bbeta\mid X,Y)=\mathcal{N}(\mmu_1, \sigma^2\SSigma_1)$
    with parameters:
    \begin{align}
        \label{posterior_mu}&\SSigma_1^{-1}=\SSigma_0^{-1}+\XTX&&\\
        \label{posterior_mu1}&\mmu_1=\SSigma_1\left(\XTX \widehat\bbeta + \SSigma_0^{-1}\mmu_0\right)&&\\
        \label{posterior_mu2}&\bbetahat=\XTXi X^TY&&
    \end{align}
\end{proposition}
\begin{proposition}[Posterior predictive distribution with known variance]\label{posterior_pred}
    For an observation $(x, y)$ and its expectation $(x, \yhat\eq x\cdot\bbeta)$,
    posterior conditional distributions of $y$ and $\yhat$ are normal with parameters given below:
    \begin{align}
        \label{posterior_y}&f(y\mid x, X, Y)=\mathcal{N}\left(x\mmu_1, \sigma^2\left(1+x\SSigma_1 x^T\right)\right)&&\\
        \label{posterior_y1}&f(\widehat y\mid x, X, Y)=\mathcal{N}\left(x\mmu_1, \sigma^2 x\SSigma_1 x^T\right)&&
    \end{align}
\end{proposition}
\begin{proposition}[Bayesian regression under known vairance and non-informative prior]\label{non_inform}
    If the prior on parameter~$\bbeta$ is non-informative, i.e. $\mmu_0=0$ and $\SSigma_0^{-1}=0$,
    then posterior distribution of model parameters and predictive distributions recover standard OLS formulas:
    \begin{align}
        \label{non_inform_posterior_mu}&\bbeta\sim\mathcal{N}\left(\XTXi\XTY,\sigma^2\XTXi\right)&&\\
        \label{non_inform_posterior_y}&f(y\mid x, X, Y)=\mathcal{N}\left(x\XTXi\XTY, \sigma^2\left(1+x\XTXi x^T\right)\right)&&\\
        \label{non_inform_posterior_y1}&f(\widehat y\mid x, X, Y)=\mathcal{N}\left(x\XTXi\XTY, \sigma^2 x\XTXi x^T\right)&&
    \end{align}
\end{proposition}
\begin{proof}[Proof of Proposition \ref{posterior}]
Define for convenience $\LLambda_0\eq\SSigma_0^{-1}$. Using $f(\beta\mid X, Y)\propto f(X, Y\mid \bbeta)f(\bbeta)$ and taking logarithms one has:
\begin{equation*}\begin{split}
    \ln f(\bbeta\mid X, Y)+\const=-\left(\frac n 2 \ln{\sigma^2}+\frac n 2\ln 2\pi+\frac 1 2 \ln \SSigma_0+\frac k 2 \ln 2\pi\right)-&&\\
    -\frac 1{2\sigma^2}(Y-X\bbeta)^T(Y-X\bbeta)-\frac 1{2\sigma^2}(\bbeta-\mmu_0)^T\LLambda_0(\bbeta-\mmu_0)=&&\\
    =-\left(\frac n 2\ln{\sigma^2}+\frac n 2\ln 2\pi+\frac 1 2 \ln \SSigma_0+\frac k 2 \ln 2\pi\right)-&&\\
    -\frac 1{2\sigma^2}\left(\bbeta^T\left(X^TX+\LLambda_0\right)\bbeta-(Y^TX+\mmu_0^T\LLambda_0)\bbeta-
    \bbeta^T\left(X^TY+\LLambda_0\mmu_0\right)+Y^TY+\mmu_0^T\LLambda_0\mmu_0\right)&&
\end{split}\end{equation*}
It suffices to show that this expression is a quadratic form $-\frac 1 {2\sigma^2}(\bbeta-\mmu_1)^T\LLambda_1(\bbeta-\mmu_1)$
up to an additive term independent of $\bbeta$. Here $\mmu_1$ and $\LLambda_1$ are as in \ref{posterior_mu}-\ref{posterior_mu2}.
This follows immediately from the lemma on completing squares:
\begin{lemma}\label{lemmaQ}
    For any symmetric quadratic form $Q$, liner form $L$ and vector $v$:
    \begin{equation*}
        v^TQv-v^TL-L^Tv=(v-Q^{-1}L)^TQ(v-Q^{-1}L)-L^TQ^{-1}L
    \end{equation*}
\end{lemma}
\end{proof}
\begin{proof}[Proof of Proposition \ref{posterior_pred}]
\noindent To find the posterior predictive distribution \ref{posterior_y}, we integrate out parameter $\bbeta$:
\begin{equation*}\begin{split}
f(y\mid x,X,Y)=\int\displaylimits_{\bbeta\in\RR^k}f(y\mid x,\bbeta)&f(\bbeta\mid X,Y)d\bbeta=\\
=\int\displaylimits_{\bbeta\in\RR^k}&\frac {\left(\det\LLambda_1\right)^{\frac 12}}{\left(2\pi\sigma^2\right)^{\frac{1+k}2}}
\exp\left(-\frac 1{2\sigma^2}\left((y-x\beta)^2+(\bbeta-\mmu_1)^T\LLambda_1(\bbeta-\mmu_1)\right)\right)d\bbeta
\end{split}\end{equation*}
Set $\bbetatilde\eq\bbeta-\mmu_1$ and $\ytilde\eq y-x\mmu_1$. The expession under the exponential can be rewritten as:
\begin{equation*}\begin{split}
    (y-x&\beta)^2+(\bbeta-\mmu_1)^T\LLambda_1(\bbeta-\mmu_1)=
    \left(\ytilde-x\bbetatilde\right)^2+\bbetatilde^T\LLambda_1\bbetatilde=\\
    &=\ytilde^2-\ytilde x\left(\LLambda_1+x^Tx\right)^{-1}x^T\ytilde+
    \left(\bbetatilde-\left(\LLambda_1+x^Tx\right)^{-1}x^T\ytilde\right)^T\left(\LLambda_1+x^Tx\right)
    \left(\bbetatilde-\left(\LLambda_1+x^Tx\right)^{-1}x^T\ytilde\right)
\end{split}\end{equation*}
Using that for any positive definite form $Q$ the integral $\exp(-(\bbeta-\boldsymbol{v})^TQ(\bbeta-\boldsymbol{v}))$ is independent of $\boldsymbol{v}\in\RR^k$,
we find that up to a multiplicative constant:
\begin{equation*}
    f(y\mid x, X, Y)=\const\cdot \exp\left(-\frac {\ytilde^2}{2\sigma^2}\left(1-x\left(\LLambda_1+x^Tx\right)^{-1}x^T\right)\right)=
    \const\cdot \exp\left(-\frac {\ytilde^2}{2\sigma^2}\left(1+x\SSigma_1 x^T\right)^{-1}\right)
\end{equation*}
Consequently, $\ytilde=y-x\mmu_1$ is normally distributed with variance as prescribed by \ref{posterior_y}.
The second equality above follows from \href{https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula}{Sherman-Morrison formula}.
\begin{theorem}[Sherman-Morrison formula]
    Suppose $A\in\RR^{k\times k}$ is an invertible matrix and $u,v\in \RR^k$ are vectors. Then $A+uv^T$  is invertible iff $1+v^TA^{-1}u\neq 0$.
    In this case,
    \begin{align}
        \left(A+uv^T\right)^{-1}=A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}\\
        v^T\left(A+uv^T\right)^{-1}u=\frac{v^TA^{-1}u}{1+v^TA^{-1}u}
    \end{align}
\end{theorem}
The proof of \ref{posterior_y1} may be seen as a direct consequence of the general result on convolution of multivariate normal distributions.
The result is well-known and is usually demonstrated using Fourier transform.
We give an ``elementary'' proof in Appendix \ref{appendix_gauss_conv}
\end{proof}
\begin{proof}[Proof of Proposition \ref{non_inform}]
Straightforward by substituting $\mmu_0=0$ and $\SSigma_0^{-1}=0$ in propositions \ref{posterior} and \ref{posterior_pred}
\end{proof}

\section{Inverse Gamma Prior for Variance Scale Parameter}
\begin{proposition}[Posterior paramer distribution]\label{posterior_var}
    Under assumptions with unkown variance \ref{assumptions}, the posterior distribution decomposes as
    $f\left(\bbeta,\sigma^2\mid X, Y\right)=f\left(\bbeta\mid X, Y, \sigma^2\right)\cdot f\left(\sigma^2\mid X, Y\right)$ where the conditional posterior distribution $f(\bbeta\mid X, Y, \sigma^2)=\mathcal{N}(\mmu_1, \sigma^2\SSigma_1)$
    is normal with parameters as in \ref{posterior_mu}-\ref{posterior_mu2} and $f\left(\sigma^2\mid X, Y\right)=\invGamma\left(a_1, b_1\right)$
    with parameters:
    \begin{align}
        \label{posterior_sigsq}&a_1=a_0+\frac n 2&&\\
        \label{posterior_sigsq2}&b_1=b_0+\frac 12\left(Y^TY+\mmu_0\SSigma_0^{-1}\mmu_0^T-\mmu_1\SSigma_1^{-1}\mmu_1^T\right)&&
    \end{align}
    In particular, for $\LLambda_0=0$ the increment in the update of $b_0$ is the residual sum of squares for OLS regression:
    \begin{equation*}
        b_1=b_0+\frac 12 \left(Y^TY-Y^TX\XTXi X^TY\right)
    \end{equation*}
\end{proposition}

\begin{proposition}[Posterior predictive distribution]\label{posterior_var_pred}
    For an observation $(x, y)$ and its expectation $(x, \yhat\eq x\cdot\bbeta)$,
    posterior conditional distributions of $y$ and $\yhat$ are location-scale $t$-distributions with parameters given below:
    \begin{align}
        \label{posterior_var_y}&f(y\mid x, X, Y)=lst\left(x\mmu_1, \frac{b_1}{a_1}\left(1+x\SSigma_1 x^T\right), 2a_1\right)&&\\
        \label{posterior_var_y1}&f(\widehat y\mid x, X, Y)=lst\left(x\mmu_1, \frac{b_1}{a_1} x\SSigma_1 x^T, 2a_1\right)&&
    \end{align}
\end{proposition}
\begin{proposition}[Bayesian regression and non-informative prior]\label{non_inform_var}
    Assume that prior on parameters has form $\displaystyle f(\bbeta,\sigma^2)\propto\frac 1{\sigma^2}$. Then posterior distribution
    of model parameters and predictive distributions recover standard OLS fomulas:
    \begin{align}
        \label{non_inform_posterior_var_sigma2}&\widehat{\sigma^2}=\frac 1{n-k}\left(Y^TY-Y^TX\XTXi X^TY\right)&&\\
        \label{non_inform_posterior_var_mu}&\bbeta\sim t_{n-k}\left(\XTXi\XTY,\widehat{\sigma^2}\XTXi\right)&&\\
        \label{non_inform_posterior_var_y}&f(y\mid x, X, Y)=t_{n-k}\left(x\XTXi\XTY,\widehat{\sigma^2}\left(1+x\XTXi x^T\right)\right)&&\\
        \label{non_inform_posterior_var_y1}&f(\widehat y\mid x, X, Y)=t_{n-k}\left(x\XTXi\XTY,\widehat{\sigma^2}\ x\XTXi x^T\right)&&
    \end{align}
    More specifically, the poseteror parameter distribution can be decomposed as in Proposition \ref{posterior_var} with:
    \begin{align}
        &f\left(\bbeta\mid X, Y, \sigma^2\right)=\mathcal{N}\left(\XTXi\XTY,\sigma^2\XTXi\right)&&\\
        &f\left(\sigma^2\mid X, Y\right)=\invGamma\left(\frac{n-k}2,\frac 12 \left(Y^TY-Y^TX\XTXi X^TY\right)\right)&&
    \end{align}
\end{proposition}
\begin{lemma}\label{beta_marginal}
    Suppose the distribution of $(\bbeta,\sigma^2)\in \RR^{k}\times\RR_+$ is
    $f\left(\bbeta,\sigma^2\right)=f\left(\bbeta\mid\sigma^2\right)\cdot f\left(\sigma^2\right)$ with
    $f\left(\bbeta\mid\sigma^2\right)$ normal $\mathcal{N}\left(\mmu,\SSigma\right)$ and $f\left(\sigma^2\right)=\invGamma\left(a, b\right)$.
    Then the marginal distribution of $\bbeta$ is multivariate $t$-distribution with $2a$ degrees of freedom: $f(\bbeta) = t_{2a}\left(\mmu,\frac ba\SSigma\right)$.
\end{lemma}
\begin{proof}[Proof of Proposition \ref{posterior_var}]
Using $f(\bbeta, \sigma^2\mid X, Y)\propto f(X, Y\mid \bbeta, \sigma^2)f(\bbeta\mid\sigma^2)f(\sigma^2)$ and
substituting assumptions for model distributions one gets:
\begin{equation*}\begin{split}
    f(\bbeta,\sigma^2\mid X, Y)\propto&\frac 1 {\left(2\pi\sigma^2\right)^{\frac n 2}}
    \exp\left(-\frac 1{2\sigma^2}\left(Y-X\bbeta\right)^T\left(Y-X\bbeta\right)\right)\cdot\\
    &\cdot\frac{\left(\det\LLambda_0\right)^{\frac 12}}{\left(2\pi\sigma^2\right)^{\frac{k}2}}
    \exp\left(-\frac 1{2\sigma^2}(\bbeta-\mmu_0)^T\LLambda_0(\bbeta-\mmu_0)\right)
    \frac{{b_0}^{a_0}}{\Gamma(a_0)}\left(\sigma^2\right)^{-a_0-1}\exp\left(-\frac{b_0}{\sigma^2}\right)
\end{split}\end{equation*}
Following the proof of Proposition \ref{posterior}, completing the squares under the exponential gives:
\begin{equation*}\begin{split}
    -\frac 1{2\sigma^2}&\left(\bbeta^T\LLambda_1\bbeta-\mmu_1^T\LLambda_1\bbeta-\bbeta^T\LLambda_1\mmu_1+Y^TY+\mmu_0^T\LLambda_0\mmu_0\right)=\\
    &=-\frac 1{2\sigma^2}\left((\bbeta-\mmu_1)^T\LLambda_1(\bbeta-\mmu_1)+Y^TY+\mmu_0^T\LLambda_0\mmu_0-\mmu_1^T\LLambda_1\mmu_1\right)
\end{split}\end{equation*}
It follows that up to a constant independent of $\bbeta$ and $\sigma^2$, the posterior $f(\bbeta,\sigma^2\mid X, Y)$ can be written as:
\begin{equation*}
    \frac 1{\left(2\pi\sigma^2\right)^{\frac{k}2}}
    \exp\left(-\frac 1{2\sigma^2}(\bbeta-\mmu_1)^T\LLambda_1(\bbeta-\mmu_1)\right)
    \left(\sigma^2\right)^{-a_0-\frac n 2 -1}\exp\left(-\frac{1}{\sigma^2}\left(b_0+\frac 12 
    \left(Y^TY+\mmu_0^T\LLambda_0\mmu_0-\mmu_1^T\LLambda_1\mmu_1\right)
    \right)\right)
\end{equation*}
\end{proof}
\begin{proof}[Proof of Proposition \ref{posterior_var_pred}]
We only give the proof for predictive distribution of $y$, the result for $\widehat{y}$ is shown analogously. By definition,
\begin{equation}\label{posterior_pred_proof}\begin{split}
    f(y\mid x, X, Y)=\int\displaylimits_{\sigma^2\in\RR_+}
    \left[\int\displaylimits_{\bbeta\in\RR^k}f\left(y\mid x,\bbeta,\sigma^2\right)f\left(\bbeta\mid X,Y,\sigma^2\right)d\bbeta\right]
    f\left(\sigma^2\mid X, Y\right)d\sigma^2
\end{split}\end{equation}
Following the proof and reusing notations of Proposition \ref{posterior_pred}, the inner integral has the form:
\begin{equation*}\begin{split}
    \int\displaylimits_{\bbeta\in\RR^k}\frac{\left(\det\LLambda_1\right)^{\frac 12}}{\left(2\pi\sigma^2\right)^{\frac{1+k}2}}
    &\exp\left(-\frac 1{2\sigma^2}\left((y-x\beta)^2+(\bbeta-\mmu_1)^T\LLambda_1(\bbeta-\mmu_1)\right)\right)d\bbeta=\\
    =\int\displaylimits_{\bbeta\in\RR^k}&\frac{\left(\det\LLambda_1\right)^{\frac 12}}{\left(2\pi\sigma^2\right)^{\frac{1+k}2}}
    \exp\left(-\frac{\ytilde^2}{2\sigma^2}\left(1+x\SSigma_1 x^T\right)^{-1}\right)
    \exp\left(-\frac 1{2\sigma^2}\left((\bbeta-\boldsymbol{v})^TQ(\bbeta-\boldsymbol{v})\right)\right)d\bbeta=\\
    =&\const\cdot\frac 1{\left(\sigma^2\right)^\frac 1 2}\exp\left(-\frac{\ytilde^2}{2\sigma^2}\left(1+x\SSigma_1 x^T\right)^{-1}\right)
\end{split}\end{equation*}
where $\boldsymbol{v}\in\RR^k$ is a vector and $Q$ is a positive-definite quadratic form, both independent of $\bbeta$ and $\sigma^2$.
In the second equality we used that the $k$-th power of $\sigma^2$ cancels out after integrating the second exponential
($k$-dimensional volume scale).

Denote $s^2=\left(1+x\SSigma_1 x^T\right)$. Substituting the above expression in \ref{posterior_pred_proof} and using the explicit form
of $\invGamma(a_1, b_1)$ distribution one gets:
\begin{equation*}\begin{split}
    f(y\mid x, X, Y)\propto\int\displaylimits_{\sigma^2\in\RR_+}\frac 1{\left(\sigma^2\right)^{a_1+1+\frac 1 2}}
    \exp\left(-\frac 1{\sigma^2}\left(b_1+\frac{\ytilde^2}{2s^2}\right)\right)&d\sigma^2=\\
    =\Gamma\left(a_1+\frac 1 2\right)\left(b_1+\frac{\ytilde^2}{2s^2}\right)^{-a_1-\frac 1 2}&=
    \frac{\Gamma\left(a_1+\frac 1 2\right)}{b_1^{\frac{2a_1+1}2}}\left(1+\frac{a_1 \ytilde^2 }{2a_1 b_1 s^2}\right)^{-\frac{2a_1+1}2}
\end{split}\end{equation*}
Where the first step follows from $\displaystyle\int_{\RR_+}r^{-A-1}\exp\left(-\frac B r\right) dr=\frac{\Gamma(A)}{B^A}$ for any $A>0, B>0$.
Hence, the variable $\displaystyle\frac{a_1 \ytilde^2 }{b_1 s^2}$ follows $t$-distribution with~$2a_1$~degrees of freedom
which readily implies the proposition.
\end{proof}
\begin{proof}[Proof of Lemma \ref{beta_marginal}]
The proof is similar to the last part of proof of Proposition \ref{posterior_var_pred}. Denote for convenience $\LLambda\eq\SSigma^{-1}$.
\begin{equation*}
    \begin{split}
        f(\beta)=\int_{\sigma^2\in\RR_+}f\left(\beta,\sigma^2\right)d\sigma^2&=\\
        =\int_{\sigma^2\in\RR_+}\frac{\left(\det\LLambda\right)^{\frac 12}}{\left(2\pi\sigma^2\right)^{\frac k2}}&
        \exp\left(-\frac 1{2\sigma^2}(\bbeta-\mmu)^T\LLambda(\bbeta-\mmu)\right)\cdot \frac{{b}^{a}}
        {\Gamma(a)}\left(\sigma^2\right)^{-a-1}\exp\left(-\frac{b}{\sigma^2}\right) d\sigma^2=\\
        =\const\cdot&\int_{\sigma^2\in\RR_+}\left(\sigma^2\right)^{-a-\frac k 2 -1}\exp\left(-\frac 1{\sigma^2}
        \left(b+\frac 12(\bbeta-\mmu)^T\LLambda(\bbeta-\mmu)\right)\right)d\sigma^2=\\
        &=\const\cdot \frac{\Gamma{\left(a+\frac k 2\right)}}{\left(b+(\bbeta-\mmu)^T\LLambda(\bbeta-\mmu)\right)^{a+\frac k 2}}=
        \frac{\const \cdot b^{-a-\frac k2}\Gamma{\left(a+\frac k 2\right)}}{\left(1+\frac 1{2a}(\bbeta-\mmu)^T\left(\frac ab\LLambda\right)(\bbeta-\mmu)\right)^{\frac {2a+k}2}}
    \end{split}
\end{equation*}
It remains to recognize the expression above as the standard representation of $t_{2a}\left(\mmu,\frac ba\SSigma\right)$.
\end{proof}

\appendix\section{``Elementary'' Proof of Gaussian Convolution}\label{appendix_gauss_conv}
% next \section will use \appendix counter
\begin{proposition}
    Let $\bbeta\in\RR^k$ be a gaussian vector with distribution $\mathcal{N}(\mmu,\SSigma)$ and let $A$ be any $l\times k$ matrix.
    Then $A\bbeta\in\RR^k$ is a gaussian vector with distribution $\mathcal{N}(A\mmu,A\SSigma A^T)$
\end{proposition}
\begin{proof} We only consider the case $l=1$ with $A=a=(a_1, a_2,\ldots, a_k)$ and $A\bbeta$ is a one dimensional random variable.
Without loss of generality we may assume $a_1\neq 0$. Introduce notations:
\begin{align}
    &\yhat = a\bbeta\\
    &\ytilde=\yhat-a\mmu\\
    &a = \left(a_1,\ \aaa_{-1}\right),\ a_1\in\RR^k,\ \aaa_{-1}\in\RR^{k-1}\\
    &\bbeta = \left(\beta_1,\ \bbeta_{-1}^T\right)^T,\ \beta_1\in\RR^k,\ \bbeta_{-1}\in\RR^{k-1}\\
    &\bbetatilde=\left(\bbetatilde_1,\ \bbetatilde_{-1}^T\right)^T=\bbeta-\mmu\\
    &\LLambda=\SSigma^{-1}\\
    &\LLambda=\left(\begin{matrix}
        \lambda_{11} && \llambda_1\\
        \llambda_1^T && \LLambda_{-1}
    \end{matrix}\right)
\end{align}
We note that the volume element
$\displaystyle d\bbeta=d\beta_1\boldsymbol{d}\bbeta_{-1}=d\left(\frac{\yhat-\aaa_{-1}\bbeta_{-1}}{a_1}\right)d\bbeta_{-1}=\frac 1{a_1}d\yhat\boldsymbol{d}\bbeta{-1}$.
Thus to get the density of $\yhat$ it suffices to integrate out $\boldsymbol{d}\bbeta_{-1}$. It's easy to re-center variables for convenience using:
\begin{equation*}\begin{split}
    \left(\frac{1}{a_1}\left(\yhat-\aaa_{-1}\bbeta_{-1}\right)-\mmu_1,\ \bbeta_{-1}^T-\mmu_{-1}^T\right)=&\left(\frac{1}{a_1}\left(\yhat-a_1\mmu_1-\aaa_{-1}\left(\bbeta_{-1}-\mmu_{-1}\right)-\aaa_{-1}\mmu_{-1}\right),\ \bbeta_{-1}^T-\mmu_{-1}^T\right)=\\
    =&\left(\frac{1}{a_1}\left(\ytilde-\aaa_{-1}\bbetatilde_{-1}\right)-\mmu_1,\ \bbetatilde_{-1}^T\right)
\end{split}\end{equation*}
We will use that the integral of the exponent of a qudratic form in a shift of $\bbetatilde$ gives a multiplicative constant that
does not depend on $\ytilde$
(same idea as in the proof of Proposition \ref{posterior_pred}, see \ref{lemmaQ}):
\begin{equation*}\begin{split}
    \left(\begin{matrix}\frac 1{a_1}\left(\ytilde-\aaa_{-1}\bbetatilde_{-1}\right)\\\bbetatilde_{-1}\end{matrix}\right)^T
    &\LLambda
    \left(\begin{matrix}\frac 1{a_1}\left(\ytilde-\aaa_{-1}\bbetatilde_{-1}\right)\\\bbetatilde_{-1}\end{matrix}\right)=\\
    =\left(\begin{matrix}\frac \ytilde{a_1}\\\boldsymbol{0}\end{matrix}\right)^T
    \LLambda
    \left(\begin{matrix}\frac \ytilde{a_1}\\\boldsymbol{0}\end{matrix}\right)+
    &\left(\begin{matrix}\frac \ytilde{a_1}\\\boldsymbol{0}\end{matrix}\right)^T
    \LLambda
    \left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)\bbetatilde_{-1}+
    \bbetatilde_{-1}^T\left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)^T
    \LLambda
    \left(\begin{matrix}\frac \ytilde{a_1}\\\boldsymbol{0}\end{matrix}\right)^T+
    \bbetatilde_{-1}^T\left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)^T
    \LLambda\left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)\bbetatilde_{-1}
\end{split}\end{equation*}
Completing squares in $\bbetatilde_{-1}$ and integrating it out, we're left with an exponential of the following expression in $\ytilde$:
\begin{equation}\label{appendix_gauss_conv_eq}\begin{split}
    \left(\begin{matrix}\frac \ytilde{a_1}\\\boldsymbol{0}\end{matrix}\right)^T
    \LLambda&
    \left(\begin{matrix}\frac \ytilde{a_1}\\\boldsymbol{0}\end{matrix}\right)-
    \left(\begin{matrix}\frac \ytilde{a_1}\\\boldsymbol{0}\end{matrix}\right)^T
    \LLambda
    \left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)
    \left(
        \left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)^T
        \LLambda
        \left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)
    \right)^{-1}
    \left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)^T
    \LLambda
    \left(\begin{matrix}\frac \ytilde{a_1}\\\boldsymbol{0}\end{matrix}\right)=\\
    &=\frac{\ytilde^2}{a_1^2}\lambda_{11}-\left(-\frac{\ytilde}{a_1^2}\lambda_{11}\aaa_{-1}+\frac{\ytilde}{a_1}\llambda_1\right)
    \left(
        \left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)^T
        \LLambda
        \left(\begin{matrix}-\frac {\aaa_{-1}}{a_1}\\\boldsymbol{I}\end{matrix}\right)
    \right)^{-1}
    \left(-\frac{\ytilde}{a_1^2}\lambda_{11}\aaa_{-1}^T+\frac{\ytilde}{a_1}\llambda_1^T\right)=\\
    =\frac{\ytilde^2}{a_1^2}\lambda_{11}-&\left(-\frac{\ytilde}{a_1^2}\lambda_{11}\aaa_{-1}+\frac{\ytilde}{a_1}\llambda_1\right)
    \left(
    \frac 1{a_1^2}\aaa_{-1}^T\aaa_{-1}-\frac 1{a_1}\llambda_{-1}^T\aaa_{-1}-\frac 1{a_1}\aaa_{-1}^T\llambda_{-1}+\LLambda_{-1}
    \right)^{-1}
    \left(-\frac{\ytilde}{a_1^2}\lambda_{11}\aaa_{-1}^T+\frac{\ytilde}{a_1}\llambda_1^T\right)
\end{split}\end{equation}
The next step is to apply recognize the above expression as the inverse of a matrix using two versions of the \href{https://en.wikipedia.org/wiki/Block_matrix}{block-matrix inverse}:
\begin{lemma}
    If matrices $A$ and $D-CA^{-1}B$ are invertible then:
    \begin{equation}\label{appendix_gauss_conv_eq2}
        \left(\begin{matrix}A&&B\\C&&D\end{matrix}\right)^{-1}=
        \left(\begin{matrix}
        A^{-1}+A^{-1}B\left(D-CA^{-1}B\right)^{-1}CA^{-1}&&-A^{-1}B\left(D-CA^{-1}B\right)^{-1}\\
        -\left(D-CA^{-1}B\right)^{-1}CA^{-1}&&\left(D-CA^{-1}B\right)^{-1}
        \end{matrix}\right)
    \end{equation}
    If matrices $D$ and $A-BD^{-1}C$ are invertible then:
    \begin{equation}
        \left(\begin{matrix}A&&B\\C&&D\end{matrix}\right)^{-1}=
        \left(\begin{matrix}
        \left(A-BD^{-1}C\right)^{-1}&&-\left(A-BD^{-1}C\right)^{-1}BD^{-1}\\
        -D^{-1}C\left(A-BD^{-1}C\right)^{-1}&&D^{-1}+D^{-1}C\left(A-BD^{-1}C\right)^{-1}BD^{-1}
        \end{matrix}\right)
    \end{equation}
    If matrices $A$ and $D$ are invertible then:
    \begin{equation}
        \left(\begin{matrix}A&&B\\C&&D\end{matrix}\right)^{-1}=
        \left(\begin{matrix}\left(A-BD^{-1}C\right)^{-1}&&\boldsymbol{0}\\\boldsymbol{0}&&\left(D-CA^{-1}B\right)^{-1}\end{matrix}\right)
        \left(\begin{matrix}\boldsymbol{I}&&-BD^{-1}\\-CA^{-1}&&\boldsymbol{I}\end{matrix}\right)
    \end{equation}
\end{lemma}
Applying the above to
$A=\frac{a_1^2}{\lambda_{11}},\ B=\aaa_{-1}-\frac{a_1}{\lambda_{11}}\llambda_1,\ C=-B^T,\ D=\LLambda_{-1}-\frac 1{\lambda_{11}}\llambda_1^T\llambda_1$
we see that the right hand side of \ref{appendix_gauss_conv_eq} can be rewrittn further
\begin{equation}\begin{split}
    \ytilde^2\left(
        \frac{a_1^2}{\lambda_{11}}+
        \left(-\aaa_{-1}+\frac{a_1}{\lambda_{11}}\llambda_1\right)
        \left(\LLambda_{-1}-\frac 1{\lambda_{11}}\llambda_1^T\llambda_1\right)^{-1}
        \left(-\aaa_{-1}^T+\frac{a_1}{\lambda_{11}}\llambda_1^T\right)
        \right)^{-1}=\\
        =\ytilde^2+\left(\aaa\left(
            \left(\begin{matrix}\frac 1{\lambda_{11}}&&\boldsymbol{0}\\\boldsymbol{0}&&\boldsymbol{0}\end{matrix}\right)+
            \left(\begin{matrix}\frac 1{\lambda_{11}}\llambda_1\\-\boldsymbol{I}\end{matrix}\right)
            \left(\LLambda_{-1}-\frac 1{\lambda_{11}}\llambda_1^T\llambda_1\right)^{-1}
            \left(\begin{matrix}\frac 1{\lambda_{11}}\llambda_1^T&&-\boldsymbol{I}\end{matrix}\right)
            \right)\aaa^T\right)^{-1}
\end{split}\end{equation}
It suffices to observe that the quadratic form in $\aaa$ in the expression above equals $\SSigma$. Indeed this follows from
the formula \ref{appendix_gauss_conv_eq2} applied to $\SSigma=\LLambda^{-1}$.
\end{proof}

\end{document}